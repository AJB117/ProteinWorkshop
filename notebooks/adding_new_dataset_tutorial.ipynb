{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Protein Workshop` Tutorial, Part 3 - Adding a New Dataset\n",
    "![Datasets](../docs/source/_static/box_datasets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a custom dataset to the `Protein Workshop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new subclass of the `ProteinDataModule` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference the `CATHDataModule` below (i.e., `src/datasets/cath.py`) to fill out a custom `src/datasets/my_new_dataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class CATHDataModule(ProteinDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        batch_size: int,\n",
    "        format: str = \"mmtf\",\n",
    "        pdb_dir: Optional[str] = None,\n",
    "        pin_memory: bool = True,\n",
    "        in_memory: bool = False,\n",
    "        num_workers: int = 16,\n",
    "        dataset_fraction: float = 1.0,\n",
    "        transforms: Optional[Iterable[Callable]] = None,\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = Path(path)\n",
    "        self.raw_dir = self.data_dir / \"raw\"\n",
    "        self.processed_dir = self.data_dir / \"processed\"\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "\n",
    "        if transforms is not None:\n",
    "            self.transform = self.compose_transforms(\n",
    "                omegaconf.OmegaConf.to_container(\n",
    "                    transforms,\n",
    "                    resolve=True\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "        self.in_memory = in_memory\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.pin_memory = pin_memory\n",
    "        self.num_workers = num_workers\n",
    "        self.format = format\n",
    "        self.pdb_dir = pdb_dir\n",
    "\n",
    "        self.dataset_fraction = dataset_fraction\n",
    "        self.excluded_chains: List[str] = self.exclude_pdbs()\n",
    "\n",
    "    def download(self):\n",
    "        self.download_chain_list()\n",
    "\n",
    "    def parse_labels(self):\n",
    "        pass\n",
    "\n",
    "    def exclude_pdbs(self):\n",
    "        return []\n",
    "\n",
    "    def download_chain_list(self):  # sourcery skip: move-assign\n",
    "        URL = \"http://people.csail.mit.edu/ingraham/graph-protein-design/data/cath/chain_set_splits.json\"\n",
    "        if not os.path.exists(self.data_dir / \"chain_set_splits.json\"):\n",
    "            logger.info(\"Downloading dataset index file...\")\n",
    "            wget.download(URL, str(self.data_dir / \"chain_set_splits.json\"))\n",
    "        else:\n",
    "            logger.info(\"Found existing dataset index\")\n",
    "\n",
    "    @functools.lru_cache\n",
    "    def parse_dataset(self) -> Dict[str, List[str]]:\n",
    "        fpath = self.data_dir / \"chain_set_splits.json\"\n",
    "\n",
    "        with open(fpath, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        self.train_pdbs = data[\"train\"]\n",
    "        logger.info(f\"Found {len(self.train_pdbs)} chains in training set\")\n",
    "        logger.info(\"Removing obsolete PDBs from training set\")\n",
    "        self.train_pdbs = [pdb for pdb in self.train_pdbs if pdb[:4] not in self.obsolete_pdbs.keys()]\n",
    "        logger.info(f\"{len(self.train_pdbs)} remaining training chains\")\n",
    "\n",
    "        logger.info(f\"Sampling fraction {self.dataset_fraction} of training set\")\n",
    "        fraction = int(self.dataset_fraction * len(self.train_pdbs))\n",
    "        self.train_pdbs = random.sample(self.train_pdbs, fraction)\n",
    "\n",
    "        self.val_pdbs = data[\"validation\"]\n",
    "        logger.info(f\"Found {len(self.val_pdbs)} chains in validation set\")\n",
    "        logger.info(\"Removing obsolete PDBs from validation set\")\n",
    "        self.val_pdbs = [pdb for pdb in self.val_pdbs if pdb[:4] not in self.obsolete_pdbs.keys()]\n",
    "        logger.info(f\"{len(self.val_pdbs)} remaining validation chains\")\n",
    "\n",
    "        self.test_pdbs = data[\"test\"]\n",
    "        logger.info(f\"Found {len(self.test_pdbs)} chains in test set\")\n",
    "        logger.info(\"Removing obsolete PDBs from test set\")\n",
    "        self.test_pdbs = [pdb for pdb in self.test_pdbs if pdb[:4] not in self.obsolete_pdbs.keys()]\n",
    "        logger.info(f\"{len(self.test_pdbs)} remaining test chains\")\n",
    "        return data\n",
    "\n",
    "    def train_dataset(self):\n",
    "        if not hasattr(self, \"train_pdbs\"):\n",
    "            self.parse_dataset()\n",
    "        pdb_codes = [pdb.split(\".\")[0] for pdb in self.train_pdbs]\n",
    "        chains = [pdb.split(\".\")[1] for pdb in self.train_pdbs]\n",
    "\n",
    "        return ProteinDataset(\n",
    "            root=str(self.data_dir),\n",
    "            pdb_dir=self.pdb_dir,\n",
    "            pdb_codes=pdb_codes,\n",
    "            chains=chains,\n",
    "            transform=self.transform,\n",
    "            format=self.format,\n",
    "            in_memory=self.in_memory\n",
    "        )\n",
    "\n",
    "    def val_dataset(self) -> ProteinDataset:\n",
    "        if not hasattr(self, \"val_pdbs\"):\n",
    "            self.parse_dataset()\n",
    "\n",
    "        pdb_codes = [pdb.split(\".\")[0] for pdb in self.val_pdbs]\n",
    "        chains = [pdb.split(\".\")[1] for pdb in self.val_pdbs]\n",
    "\n",
    "        return ProteinDataset(\n",
    "            root=str(self.data_dir),\n",
    "            pdb_dir=self.pdb_dir,\n",
    "            pdb_codes=pdb_codes,\n",
    "            chains=chains,\n",
    "            transform=self.transform,\n",
    "            format=self.format,\n",
    "            in_memory=self.in_memory\n",
    "        )\n",
    "\n",
    "    def test_dataset(self) -> ProteinDataset:\n",
    "        if not hasattr(self, \"test_pdbs\"):\n",
    "            self.parse_dataset()\n",
    "        pdb_codes = [pdb.split(\".\")[0] for pdb in self.test_pdbs]\n",
    "        chains = [pdb.split(\".\")[1] for pdb in self.test_pdbs]\n",
    "\n",
    "        return ProteinDataset(\n",
    "            root=str(self.data_dir),\n",
    "            pdb_dir=self.pdb_dir,\n",
    "            pdb_codes=pdb_codes,\n",
    "            chains=chains,\n",
    "            transform=self.transform,\n",
    "            format=self.format,\n",
    "            in_memory=self.in_memory\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> ProteinDataLoader:\n",
    "        if not hasattr(self, \"train_ds\"):\n",
    "            self.train_ds = self.train_dataset()\n",
    "        return ProteinDataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> ProteinDataLoader:\n",
    "        if not hasattr(self, \"val_ds\"):\n",
    "            self.val_ds = self.val_dataset()\n",
    "        return ProteinDataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> ProteinDataLoader:\n",
    "        if not hasattr(self, \"test_ds\"):\n",
    "            self.test_ds = self.test_dataset()\n",
    "        return ProteinDataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new data config file to accompany the custom `MyNewDataModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference the `CATH` config below (i.e., `configs/dataset/cath.yaml`) to fill out a custom `configs/dataset/my_new_dataset.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "datamodule:\n",
    "  _target_: \"src.datasets.cath.CATHDataModule\"\n",
    "  path: ${env.paths.data}/cath/ # Directory where the dataset is stored\n",
    "  pdb_dir: ${env.paths.data}/pdb/ # Directory where raw PDB/mmtf files are stored\n",
    "  format: \"mmtf\" # Format of the raw PDB/MMTF files\n",
    "  num_workers: 4 # Number of workers for dataloader\n",
    "  pin_memory: True # Pin memory for dataloader\n",
    "  batch_size: 32 # Batch size for dataloader\n",
    "  dataset_fraction: 1.0 # Fraction of the dataset to use\n",
    "  transforms: ${transforms} # Transforms to apply to dataset examples\n",
    "num_classes: 23 # Number of classes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use new dataset as either a pre-training or fine-tuning corpus, with or without full-atom context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. tools\n",
    "import os\n",
    "\n",
    "# Hydra tools\n",
    "import hydra\n",
    "\n",
    "from hydra.compose import GlobalHydra\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "from src.constants import HYDRA_CONFIG_PATH\n",
    "from src.utils.notebook import init_hydra_singleton\n",
    "\n",
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=schnet\", \"task=inverse_folding\", \"dataset=my_new_dataset\", \"features=ca_angles\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the custom dataset using the designed config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.configs import config\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "datamodule = hydra.utils.instantiate(cfg.dataset.datamodule)\n",
    "datamodule.setup(\"train\")\n",
    "dl = datamodule.train_dataloader()\n",
    "\n",
    "for i in dl:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Either pre-train or fine-tune a model using the custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetune import finetune\n",
    "from src.train import train_model\n",
    "\n",
    "# train_model(cfg)  # Pre-train a model using the selected data\n",
    "# finetune(cfg)  # Fine-tune a model using the selected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconfigure the custom dataset to use side-chain atom context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_base = \"1.2\"  # Note: Need to update whenever Hydra is upgraded\n",
    "init_hydra_singleton(reload=True, version_base=version_base)\n",
    "\n",
    "path = HYDRA_CONFIG_PATH\n",
    "rel_path = os.path.relpath(path, start=\".\")\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(rel_path, version_base=version_base)\n",
    "\n",
    "cfg = hydra.compose(config_name=\"train\", overrides=[\"encoder=schnet\", \"task=inverse_folding\", \"dataset=my_new_dataset\", \"features=ca_sc\", \"+aux_task=none\"], return_hydra_config=True)\n",
    "\n",
    "# Note: Customize as needed e.g., when running a sweep\n",
    "cfg.hydra.job.num = 0\n",
    "cfg.hydra.job.id = 0\n",
    "cfg.hydra.hydra_help.hydra_help = False\n",
    "cfg.hydra.runtime.output_dir = \"outputs\"\n",
    "\n",
    "HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that side-chain torsions are now available as feature inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.configs import config\n",
    "\n",
    "cfg = config.validate_config(cfg)\n",
    "\n",
    "datamodule = hydra.utils.instantiate(cfg)\n",
    "datamodule.setup(\"train\")\n",
    "dl = datamodule.train_dataloader()\n",
    "\n",
    "for i in dl:\n",
    "    print(i)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
