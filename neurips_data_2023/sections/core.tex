\section{Benchmark}
The overarching goal of the benchmark is to effectively cover the design space of protein structure representation learning methods. To achieve this, the benchmark is highly modular by design, enabling evaluation of different combinations of structural encoders, protein featurisation schemes, and auxiliary tasks over a wide range of both supervised and unsupervised tasks.

\subsection{Featurisation Schemes}
Protein structures are typically represented as graphs, with researchers typically opting to use C$\alpha$ atoms as nodes as full atom representations can quickly become computationally intractable due to the explosion in graph size. However, this is a lossy representation, with much of the structural detail, such as backbone and sidechain structure, being only implicitly encoded. Due to the computational burden incurred by operating on full-atom node representations, we focus primarily on C$\alpha$ based-graph representations, investigating featurisation strategies to incorporate higher-level structural information though we do provide utilities to enable users to work with backbone and full-atom graphs.



\begin{table*}[!ht]
    \centering
    \caption{Overview of supervised tasks and datasets}
    \begin{adjustbox}{max width=\linewidth}
        \begin{tabular}{lcccccc}
        \toprule
        \textbf{Task} & \textbf{Category} & \textbf{Dataset Origin} & \textbf{Structures} &  \textbf{\# Train} & \textbf{\# Validation} & \textbf{\# Test} \\
        \hline
        Gene Ontology Prediction & Protein classification & \citet{Gligorijevi2021} & Experimental \\
        Fold Prediction & Protein classification & Houe et al. & Experimental
        \end{tabular}
    \end{adjustbox}
    \vspace{-1em}
\end{table*}

\subsection{Pre-training Tasks}
We provide a comprehensive suite of pre-training tasks. Broadly, these tasks can be categorised into: masked-attribute prediction, denoising-based and contrastive learning based tasks. Each of these tasks can be used as a primary training objective, e.g. in a pre-training set-up or as auxiliary tasks in a downstream supervised task.

\subsubsection{Denoising Tasks}


\paragraph{Sequence Denoising} The benchmark contains two sequence denoising variations. The first is based on mutating a fraction of the residues to a random amino acid and tasking the model with recovering the uncorrupted sequence. The second is a masked residue prediction task, where a fraction of the residues are altered to a mask value and the model is tasked to recover the uncorrupted sequence.

\paragraph{Structure Denoising} We provide two structure-based denoising tasks: coordinate denoising and torsional denoising. In the coordinate denoising task, noise is sampled from a normal or uniform distribution and scaled by noise factor, $\nu \in \Re^3$, and applied to each of the atom coordinates in the structure to ensure structural features, such as backbone or sidechain torsion angles, are also corrupted. The model can then be tasked with predicting either the per-node noise or the original uncorrupted coordinates. For the torsional denoising variant, the noise is applied to the backbone torsion angles and the cartesian coordinates are recomputed using pNeRF \cite{AlQuraishi2019} prior to feature computation. Similarly to the coordinate denoising task, the model can then be tasked with predicting either the per-residue angular noise or the original dihedral angles.

\paragraph{Sequence-Structure Co-Denoising} This task is a multitask formulation of the previously described structure and sequence denoising tasks, where separate output heads are used to denoise each of the modalities.

\subsubsection{Masked Attribute Prediction Tasks}


\paragraph{Edge Distance Prediction}


\paragraph{Angle Prediction}

\paragraph{Dihedral Angle Prediction}


\subsubsection{Self-supervised Tasks}
\paragraph{pLDDT Prediction}
Structure prediction models typically provide per-residue pLDDT (predicted Local Distance Difference Test) scores as local confidence measures in the quality of the prediction. We formulate a self-supervision task on predicted structures, somewhat analagous to structure quality assessment (QA), where the model is tasked with predicting the scaled residue-wise pLDDT $y \in [0, 1]$ values.



\subsection{Supervised Tasks}
We curate several structure-based and sequence-based datasets from the literature\footnote{To retain focus on \emph{protein} representation learning, we deliberately exclude commonly-used tasks based on protein-small molecule interactions as it is hard to disentangle the effect of the small molecule representation and the potential for bias \cite{Boyles2019}}. Importantly, the raw structures are retrieved directly from the PDB enabling users to provide a custom sequence of pre-processing steps, such as deprotonation or fixing missing regions which are common in experimental data.

\subsubsection{Node-level Tasks}
\paragraph{Paratope Prediction}
\paragraph{PPI Site Prediction}
\paragraph{Metal Binding Site Prediction}

\subsubsection{Graph-level Tasks}
\paragraph{Gene Ontology Prediction}
\paragraph{Remote Homology Detection}
\paragraph{Protein Structure Ranking}
\paragraph{Antibody Developability Prediction}








\subsection{Pre-training Datasets}
The benchmark contains several large corpuses of both experimental and predicted structural data that can be used for pre-training or inference.

\subsubsection{Experimental Structures}
\paragraph{PDB} We provide utilities for curating datasets directly from the Protein Data Bank \cite{Berman2000}. In addition to using the collection in its entirety, users can provide custom filters to subset and split the data using a combination of structural similarity, sequence similarity or temporal strategies. Structures can be filtered by length, number of chains, resolution, deposition date, presence/absence of particular ligands and structure determination method. We provide support for working with PDB structures in both \texttt{.pdb} and \texttt{.mmtf} format \cite{Bradley2017}, which significantly reduces the disk-space requirements for storing the data.


\paragraph{CATH} We employ the non-redundant dataset splits developed by Ingraham et al. as an additional, smaller pre-training dataset.




\subsubsection{Predicted Structures}
We provide ready-to-go dataloaders for several collections of predicted structures derived from both AlphaFold2 and ESMFold. By developing a dataloaders that operate on FoldComp databases, a modestly lossy compression scheme for predicted protein structures, we are able to achieve significant reduction in disk-space requirements with no discernible performance degradation. We believe this will significantly benefit the field.






